{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from pattern.web import PDF\n",
      "import requests\n",
      "import re\n",
      "import urllib\n",
      "import os\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BENCHMARKED\n",
      "============\n",
      "\n",
      "*Analysis of the History and Future of the United States Supreme Court*\n",
      "\n",
      "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Supreme_Court_US_2010.jpg/640px-Supreme_Court_US_2010.jpg\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Overview\n",
      "\n",
      "COME BACK AND WRITE THIS ONCE WE UNDERSTAND THE BIG PICTURE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Section I: Gathering Data on the Court\n",
      "\n",
      "As a matter of legal record, the proceedings of the US Supreme Court are technically part of the public domain. However, structured representations of the court's records are almost entirely behind the paywalls of Lexis-Nexis and Westlaw. We've compiled data on the court from four separate pubically available sources that range from a well-cleaned, curated datasets to raw pdf documents of legal proceedings. Each source provides a different angle on the history of court that we can leverage in our analysis.\n",
      "\n",
      "## The Harold J. Spaeth Dataset\n",
      "The [Supreme Court Database](http://scdb.wustl.edu/index.php) is a publicly available data bank started in the 1980s by Harold J Spaeth, PhD, JD with a grant from the National Science Foundation to facilitate research on the Supreme Court. The accuracy of data has been externally validated and is used widely by legal academics for studies of Supreme Court decisions.  From its inception, the databank has expanded to document every vote made by the Supreme Court since 1946. In its current form, the data bank contains 247 variables for each vote. The variables, or features, of the dataset can be generalized into six main groups: identification, background, chronological, substantive, outcome, and voting/opinion variables. To make the database more tenable, we refined the database to fifty features of interest. Specifically, features were chosen based on their relevance to objectifying patterns within Supreme Court decisions.  Numerous features, such as 'Docket Number', 'Case Number', and 'Natural Court', were able to be removed simply because the variable represent irrelevant, procedural data that was present only due to the thoroughness of the data bank's documentation of the votes or because the variable was deemed to be supplementary to data present in the retained features. The retained features with which exploratory analysis is proceed upon include information on the origins each case brought before the Court for review, details of Judge votes/opinions, and statistics on Court composition.\n",
      "\n",
      "## Wikipedia Data on Supreme Court Justices\n",
      "The [Supreme Court](http://en.wikipedia.org/wiki/Supreme_Court_of_the_United_States) is part of the judicial branch of government, and thus not a political office. However, each just is appointed by a sitting President, with a political affiliation to a particular party. In order to gain insight into the political leanings of the court, we scraped Wikipedia for the political parties of the appointing President for each justice in the Spaeth dataset. We then added this party as a feature of our dataset.\n",
      "\n",
      "## The CourtListener API\n",
      "[CourtListener](https://www.courtlistener.com/) is a free legal search engine that is developed and maintained by the [Free Law Project](http://freelawproject.org/). CourtListener provides a [free API](https://www.courtlistener.com/api/) for access to both data in bulk and individual queries. We used CourtListener's bulk-data API to get information about every court case in the Spaeth dataset, including which other cases it cited. The data was retrieved as XML and then parsed and restructured so that it could be compiled with the Spaeth data. Using these citations, we created a network of influence among the Supreme Court opinions.\n",
      "\n",
      "## Supreme Court Briefs Made Available by the American Bar Association\n",
      "The [American Bar Association](http://www.americanbar.org) publishes the text of briefs to the Supreme Court in PDF format. Before each case the court takes, the petitioner and the respondent submit a merit brief to the court which outlines their argument. We created a script to download all of the pdfs from 2003-2009, open the documents, and parse them for citations of Supreme Court opinions. We then matched these briefs with the opinion that the court gave when the case was decided."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The Code to Make it Work\n",
      "Below is the code for gathering the data from the above sources. Note: The cells below need not be executed; we output the final data to csv files that can be read in at the start of Section II: Exploration. Some of the cells take an extremely long time to execute or may retrieve very large amounts of data; we've changed the default on these cells not to execute for safety's sake.\n",
      "\n",
      "## Source 1: Spaeth\n",
      "The \"justice centered\" (meaning it has a row for each justice's vote on each case) dataset is available for download from the [Supreme Court Database website](http://scdb.wustl.edu/data.php). We downloaded the justice centered data organized by Supreme Court case citation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir = \"data3\"\n",
      "\n",
      "if not os.path.exists(data_dir):\n",
      "    os.makedirs(data_dir)\n",
      "\n",
      "# download the spaeth dataset\n",
      "urllib.urlretrieve(\"http://scdb.wustl.edu/_brickFiles/2013_01/SCDB_2013_01_justiceCentered_Citation.csv.zip\", \n",
      "                   data_dir + \"/SCDB_2013_01_justiceCentered_Citation.csv.zip\")\n",
      "\n",
      "# unzip it (unix only)\n",
      "!unzip data_test/SCDB_2013_01_justiceCentered_Citation.csv.zip -d $data_dir\n",
      "\n",
      "# read the dataframe into pandas\n",
      "fulldf = pd.read_csv(data_dir + \"/SCDB_2013_01_justiceCentered_Citation.csv\")\n",
      "\n",
      "# write out the columns that we want to keep to a new csv\n",
      "df_to_keep = fulldf[['caseId',\n",
      "                     'docketId',\n",
      "                     'caseIssuesId',\n",
      "                     'voteId',\n",
      "                     'dateDecision',\n",
      "                     'usCite',\n",
      "                     'sctCite',\n",
      "                     'ledCite',\n",
      "                     'lexisCite',\n",
      "                     'term',\n",
      "                     'chief',\n",
      "                     'caseName',\n",
      "                     'petitioner',\n",
      "                     'petitionerState',\n",
      "                     'respondent',\n",
      "                     'respondentState',\n",
      "                     'jurisdiction',\n",
      "                     'adminAction',\n",
      "                     'adminActionState',\n",
      "                     'threeJudgeFdc',\n",
      "                     'caseOrigin',\n",
      "                     'caseOriginState',\n",
      "                     'caseSource',\n",
      "                     'caseSourceState',\n",
      "                     'lcDisagreement',\n",
      "                     'certReason',\n",
      "                     'lcDisposition',\n",
      "                     'lcDispositionDirection',\n",
      "                     'declarationUncon',\n",
      "                     'caseDisposition',\n",
      "                     'partyWinning',\n",
      "                     'precedentAlteration',\n",
      "                     'issueArea',\n",
      "                     'decisionDirection',\n",
      "                     'decisionDirectionDissent',\n",
      "                     'authorityDecision1',\n",
      "                     'authorityDecision2',\n",
      "                     'lawType',\n",
      "                     'majOpinWriter',\n",
      "                     'splitVote',\n",
      "                     'majVotes',\n",
      "                     'minVotes',\n",
      "                     'justice',\n",
      "                     'justiceName',\n",
      "                     'vote',\n",
      "                     'opinion',\n",
      "                     'direction',\n",
      "                     'majority',\n",
      "                     'firstAgreement',\n",
      "                     'secondAgreement']]\n",
      "\n",
      "df_to_keep.to_csv(data_dir + '/SCDB_2013_01_justiceCentered_Citation_Cleaned.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Archive:  data_test/SCDB_2013_01_justiceCentered_Citation.csv.zip\r\n",
        "  inflating: data3/SCDB_2013_01_justiceCentered_Citation.csv  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "61\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Dictionaries for Codebook Values\n",
      "\n",
      "The Spaeth dataset encodes categorical values using integers, so in order to make our results understandable, we need to remap these integer values back to the categories they represent. To do this we scraped the online codebook for the Spaeth dataset, and created dictionaries to make back and forth between the encoded values and the plain English values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}