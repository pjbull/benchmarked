{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pattern.web import PDF\n",
      "import requests\n",
      "import re\n",
      "import urllib\n",
      "import os\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Get Data for Supreme Court Briefs\n",
      "As part of their arguments to the Supreme Court, both the petitioner and the respondent submit written briefs to the court. We wonder if the citations in these briefs have any predictive power. Once the court has made a decision, they write an opinion. The court's opinion has citations to relevant previous cases. We can create a directed graph structure to represent the citations in the Supreme Court opinions, and using this structure can calculate metrics for the relevancy of certain cases (e.g., centrality, page rank). Does these measures of centrality in the citation represent the \"precedential value\" of a particular citation? And, if a petitioner or a respondent has citations in their briefs to more \"precedential\" cases, does this mean that a decision will be made in their favor?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting Briefs Data\n",
      "There are no compiled datasets of Supreme Court Briefs. The American Bar Association makes briefs from 2003 - 2013 available in PDF format. Below we download pdfs from 2003-2009 since these are the ones we can match with the Spaeth dataset.\n",
      "\n",
      "###WARNING: THIS CELL TAKES A VERY LONG TIME TO EXECUTE\n",
      "To run the cell, update the parameter to the call so it is are_you_sure=True."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_brief_pdfs(are_you_sure=False):\n",
      "    if not are_you_sure:\n",
      "        print \"You weren't sure, so we didn't download 1.5GB of pdfs. Smart move.\"\n",
      "        return\n",
      "    \n",
      "    # months there are supreme court sessions\n",
      "    first_year_mos = ['oct','nov','dec']\n",
      "    second_year_mos = ['jan','feb','march','april']\n",
      "    \n",
      "    # urls where brief pdfs live are of this form\n",
      "    brief_pdf = re.compile('/content[ _0-9A-Za-z/ ]+briefs[ /A-Za-z0-9\\-_]+.pdf')\n",
      "    \n",
      "    # for all the years the ABA makes available\n",
      "    for i in range(6, 9):\n",
      "        j = i+1\n",
      "        \n",
      "        # makes list of oct03...april04, etc.\n",
      "        first_half = zip(first_year_mos, len(first_year_mos)*[i])\n",
      "        second_half = zip(second_year_mos, len(second_year_mos)*[j])\n",
      "        for mo, yr in first_half + second_half:\n",
      "            \n",
      "            # get the page for this month/year combo\n",
      "            url = \"http://www.americanbar.org/content/aba/publications/preview_home/publiced_preview_briefs_{}{:02d}.html\".format(mo,yr)\n",
      "            resp = requests.get(url)\n",
      "            \n",
      "            # get the pdfs for this mo/yr combo\n",
      "            matches = re.findall(brief_pdf, resp.content)\n",
      "            \n",
      "            # if a page has no results, keep that response to test the regex\n",
      "            if len(matches) == 0:\n",
      "                stored = resp.content\n",
      "                \n",
      "            # get all the pdf links\n",
      "            for m in matches:\n",
      "                filename = m.split('/')[-1]\n",
      "                root = \"http://www.americanbar.org\"\n",
      "                local_folder = \"pdfs/{:02d}-{:02d}/\".format(i,j)\n",
      "                \n",
      "                # make a folder for this session, if we don't have one\n",
      "                if not os.path.exists(local_folder):\n",
      "                    os.makedirs(local_folder)\n",
      "                    \n",
      "                # get the pdf if we haven't dled it before\n",
      "                if not os.path.exists(local_folder + filename):\n",
      "                    # skip amicus briefs\n",
      "                    if not \"amcu\" in filename and not \"AmCu\" in filename:\n",
      "                        urllib.urlretrieve(root+m, local_folder + filename)\n",
      "\n",
      "# download all the pdfs for supreme court briefs\n",
      "get_brief_pdfs(are_you_sure=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "get_citation_from_spaeth(spaeth, docket_match)\n",
      "\n",
      "Takes a docket match of the form '02_321' and looks for that\n",
      "in the spaeth dataset.\n",
      "\n",
      "Returns the US Cite for that case.\n",
      "\"\"\"\n",
      "def get_citation_from_spaeth(spaeth, docket_match):\n",
      "    docket_match = docket_match.replace('_','-')\n",
      "    \n",
      "    # we got a docket without a year, try the possible years\n",
      "    if \"-\" not in docket_match:\n",
      "        # maybe we got 02123 instead of 02-123\n",
      "        if docket_match[0] == '0':\n",
      "            docket_match = docket_match[:2] + '-' + docket_match[2:]\n",
      "        \n",
      "        else:\n",
      "            for i in range(1, 10):\n",
      "                yr = '{:02d}-'.format(i)\n",
      "                test = spaeth[spaeth['docket']==yr + docket_match]\n",
      "                if len(test.index) == 1:\n",
      "                    docket_match = yr+docket_match\n",
      "                    break\n",
      "\n",
      "    spaeth_match = spaeth[spaeth['docket']==docket_match]    \n",
      "    if len(spaeth_match['usCite']) == 0:\n",
      "        print \"No Matches: \", docket_match\n",
      "        return None\n",
      "    \n",
      "    return spaeth_match['usCite'].values[0]\n",
      "\n",
      "\"\"\"\n",
      "Use the Spaeth datset to map the pdf filenames\n",
      "to case citations.\n",
      "\n",
      "Note: We discard Amicus Curiae briefs, since we expect\n",
      "that the influential cases will be cited in the merit briefs.\n",
      "\"\"\"\n",
      "def get_docket_from_filename(spaeth, filename, folder=None):\n",
      "    # Remove Amicus Curiae briefs and appendices\n",
      "    if \"AmCu\" in filename or \"amcu\" in filename or \"Appendix\" in filename:\n",
      "        return None\n",
      "    \n",
      "    # simple case is docket_id then Pet or Resp, e.g. 03_243Pet.pdf\n",
      "    docket_id_regex = re.compile(\"[0-9]+_[0-9]+(?=[_]?[PR])\", re.I)\n",
      "    match = re.findall(docket_id_regex, filename)\n",
      "    if len(match) == 1:\n",
      "        return get_citation_from_spaeth(spaeth, match[0])\n",
      "    else:\n",
      "        # check for Pet or Resp later in the match, e.g., 03_243FinalRespondent.pdf\n",
      "        docket_id_regex2 = re.compile(\"[0-9]+_[0-9]+(?=[A-Z_]+[Resp|Pet])\", re.I)\n",
      "        match = re.findall(docket_id_regex2, filename)\n",
      "        if len(match) == 1:\n",
      "            return get_citation_from_spaeth(spaeth, match[0])\n",
      "        \n",
      "        # Check if we have the docket id, but not the year\n",
      "        docket_id_regex2 = re.compile(\"[0-9]+(?=[A-Z_]+[Resp|Pet])\", re.I)\n",
      "        match = re.findall(docket_id_regex2, filename)\n",
      "        if len(match) == 1:\n",
      "            return get_citation_from_spaeth(spaeth, match[0])\n",
      "        else:\n",
      "            print \"Discarded: \" + filename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_citations(df, brief_text):\n",
      "    cite_reg = re.compile('[0-9]+ U.S. [0-9]+')\n",
      "    for cite in brief_text:\n",
      "        re.findall(cite_reg, brief_text.string)\n",
      "\n",
      "spaeth_df = pd.read_csv('data/SCDB_2013_01_caseCentered_Docket.csv')\n",
      "\n",
      "cite_reg = re.compile('[0-9]+ U.S. [0-9]+')\n",
      "resp_re = re.compile('Resp', re.I)\n",
      "\n",
      "for i in range(3, 6):\n",
      "    us_cites = []\n",
      "    pet_resps = []\n",
      "    citations = []\n",
      "    j = i+1\n",
      "    local_folder = \"pdfs/{:02d}-{:02d}/\".format(i,j)\n",
      "    print \"========== \", local_folder, \" ===========\"\n",
      "    for brief_pdf in os.listdir(local_folder):\n",
      "        cite = get_docket_from_filename(spaeth_df, brief_pdf, folder=local_folder)\n",
      "        if cite is not None:            \n",
      "            # pull citations from the text\n",
      "            try:\n",
      "                brief_text = PDF(local_folder + brief_pdf)\n",
      "                citations.append(re.findall(cite_reg, brief_text.string))\n",
      "                us_cites.append(cite)\n",
      "                \n",
      "                # 0 if respondent, 1 if petitioner\n",
      "                pet_resp = 0 if len(re.findall(resp_re, brief_pdf)) > 0 else 1\n",
      "                pet_resps.append(pet_resp)\n",
      "            except:\n",
      "                pass\n",
      "        del(brief_pdf)\n",
      "    \n",
      "    new_df = pd.DataFrame(dict({'us_cite':us_cites,'pet_resp':pet_resps,'citations':citations}))\n",
      "    print new_df.head()\n",
      "    new_df.to_csv('cite_graph_{:02d}_{:02d}.csv'.format(i,j))\n",
      "    \n",
      "    del(us_cites)\n",
      "    del(pet_resps)\n",
      "    del(citations)\n",
      "    del(new_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "==========  pdfs/03-04/  ===========\n",
        "Discarded: .DS_Store\n",
        "No Matches: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1019\n",
        "No Matches:  1019\n",
        "No Matches: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11309\n",
        "No Matches:  11309\n",
        "Discarded: BanksResp.pdf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Discarded: CastroPet.pdf\n",
        "Discarded: CastroReply.pdf\n",
        "Discarded: GantReplyBr.pdf\n",
        "Discarded: GantSupp.pdf\n",
        "Discarded: GrohReply.pdf\n",
        "Discarded: MdExceptions.pdf\n",
        "No Matches: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 03-1183\n",
        "Discarded: VerizonPet.pdf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Discarded: VerizonReply.pdf\n",
        "Discarded: VirginiaReplytoExceptions.pdf\n",
        "                                           citations  pet_resp       us_cite\n",
        "0  [492 U.S. 302, 532 U.S. 782, 492 U.S. 302, 532...         1  542 U.S. 274\n",
        "1  [413 U.S. 266, 532 U.S. 318, 116 U.S. 616, 267...         0  541 U.S. 149\n",
        "2  [425 U.S. 682, 488 U.S. 428, 473 U.S. 234, 376...         1  541 U.S. 677\n",
        "3  [425 U.S. 682, 257 U.S. 184, 436 U.S. 604, 488...         1  541 U.S. 677\n",
        "4  [425 U.S. 682, 436 U.S. 604, 488 U.S. 428, 376...         0  541 U.S. 677\n",
        "==========  pdfs/04-05/  ===========\n",
        "Discarded: .DS_Store\n",
        "Discarded: 04_70Exxon.pdf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Discarded: ColoradoReplytoExceptions.pdf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Discarded: KansasExceptions.pdf\n",
        "Discarded: KansasSurReply.pdf\n",
        "                                           citations  pet_resp       us_cite\n",
        "0  [170 U.S. 655, 336 U.S. 465, 154 U.S. 51, 179 ...         1   543 U.S. 14\n",
        "1  [524 U.S. 214, 510 U.S. 443, 219 U.S. 186, 336...         1   543 U.S. 14\n",
        "2  [359 U.S. 297, 309 U.S. 638, 513 U.S. 219, 519...         0   543 U.S. 14\n",
        "3  [511 U.S. 1051, 519 U.S. 213, 494 U.S. 152, 53...         1  543 U.S. 157\n",
        "4  [525 U.S. 432, 510 U.S. 355, 525 U.S. 432, 517...         1  543 U.S. 157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "==========  pdfs/05-06/  ===========\n",
        "Discarded: .DS_Store\n",
        "No Matches: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 04-66\n",
        "No Matches:  04-66\n",
        "                                           citations  pet_resp       us_cite\n",
        "0  [338 U.S. 160, 365 U.S. 610, 394 U.S. 731, 497...         1  547 U.S. 103\n",
        "1  [542 U.S. 656, 496 U.S. 226, 536 U.S. 822, 512...         1  546 U.S. 418\n",
        "2  [387 U.S. 136, 415 U.S. 36, 532 U.S. 275, 520 ...         1  547 U.S. 512\n",
        "3  [530 U.S. 1229, 434 U.S. 412, 496 U.S. 384, 51...         1  546 U.S. 132\n",
        "4  [481 U.S. 739, 506 U.S. 1011, 514 U.S. 143, 37...         1  546 U.S. 320"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}